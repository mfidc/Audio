
- Live continuous Audio stream recording

- Sound analysis:
     
    - simple Silero VAD
    - spectral analysis
    - Energy based analysis
    - Zero Crossing Rate
    - Mel-Frequency Cepstral Coefficients

        Individual Analysis Wrappers
        Each return a score or flag, extract different informations
        Weighted scoring system?


- Real-Time Adaptive Noise Reduction

- Dynamic Buffer Management for Transcription
    - Dynamic Buffer Size Adjustment based on speechpatterns and context
    - Intelligent Overlap handling

- Post Transcription Processing
    - Repetition removal
    - Intelligent Concatenation

Detailed Plan for Audio Input Layer
1. Audio Capture and Preprocessing
Microphone Configuration:
Set up the microphone to capture audio at 16kHz.
Position the microphone optimally to capture clear audio.
Real-Time Preprocessing Pipeline:
Implement real-time noise reduction.
Apply audio normalization for consistent audio levels.
Introduce a dynamic range compression to balance quiet and loud sounds.
2. Chunk-Based Audio Processing
Audio Segmentation:
Implement voice activity detection (VAD) to identify active speech segments.
Segment the continuous audio stream into 3-second chunks based on VAD cues.
Buffer Management:
Design a FIFO (First-In-First-Out) buffer to manage audio chunks.
Handle overlaps between chunks to ensure continuity in speech recognition.
3. Speech Recognition
Transcriber Class Enhancement:
Adapt the Transcriber class to process 3-second audio chunks.
Ensure efficient sequential processing of chunks.
4. Feature Extraction and Enhancement
Voice Characteristics Analysis:
Extract features like pitch, volume, and speech rate.
These features can be used to detect changes in the speaker's emotional state or stress levels.
Environmental Sound Classification:
Implement basic environmental sound classification to identify background noises (e.g., traffic, music, etc.).
This information can be used to adapt the noise reduction algorithms dynamically.
Modular Design for Future Enhancements:
Design the feature extraction module to be easily extendable for future enhancements like speaker diarization or advanced emotion detection.
5. Real-Time Performance and Efficiency
Optimization for High-Performance Hardware:
Utilize the GPU (Nvidia 3090) for intensive processing tasks.
Optimize the use of the 24-core i9 processor for parallel processing where applicable.
Latency Monitoring:
Implement a system to monitor and minimize the latency in the audio processing pipeline.
Aim for near real-time performance with minimal delay between speech input and processing.
6. Modular and Scalable Architecture
Modularity:
Keep each component (preprocessing, chunk processing, speech recognition, feature extraction) modular for easy upgrades or changes.
Scalability:
Design the system with scalability in mind, allowing for future expansion like additional microphones or more complex processing capabilities.

Constraints:

-Hardware Limitations: using a USB conference microphone with a 16kHz sampling rate and 1-channel output
-Model Compatibility: Utilizing the Faster Whisper model large-v3 for transcription, which does not natively support real-time streaming input.
    ->Therefore we need this big wrapper around it. Goal should be to normally send about 3-10 seconds of audio. (maybe + overlap)
-Environmental Variability: The system must adapt to various acoustic environments, from quiet rooms to noisy outdoor settings.
-Speech Variability: Must handle different speech patterns, accents, and volumes, including soft speech that is far from the microphone and Long thought pauses when talking about complex topics for example




    

    